---
title: "Government Capacity and Disaster Recovery: A Survival Analysis of CDBG-DR Fund Management"
---


# Abstract {.unnumbered}

Government administrative capacity significantly affects disaster recovery program completion timing. Analysis of 152 grantee-disaster pairs across 78 jurisdictions (2003-2023) using Cox Proportional Hazards and Accelerated Failure Time models reveals that disbursement capacity---the ability to convert obligated funds to disbursements---predicts completion timing (HR = 4.37, p = 0.006), reducing expected completion time by 84%. Expenditure capacity shows no independent effect. These results identify early-stage fund conversion as the critical bottleneck in disaster recovery administration, suggesting that HUD monitoring should prioritize disbursement efficiency over expenditure activities. With 73.7% of programs incomplete at the 95% threshold, survival analysis provides appropriate estimation by incorporating censored observations.


# Evidence for Practice {.unnumbered}

- **Disbursement capacity is the critical bottleneck**: The ability to convert federal obligations into disbursements predicts recovery completion timing, while expenditure efficiency does not differentiate performance.

- **Early-stage processes matter most**: Procurement, environmental review, and program startup---the processes that enable disbursement---should be the primary focus of technical assistance and capacity building.

- **Simple metrics are effective**: A single disbursement ratio provides strong prediction of completion timing, suggesting a parsimonious approach to performance monitoring.

- **Most programs remain incomplete**: At the 95% completion threshold, only 26% of grantee-disaster pairs have finished, highlighting the long timelines inherent in CDBG-DR implementation.

- **Survival analysis provides unbiased estimates**: Traditional methods that exclude incomplete programs lose 74% of observations; proper censoring treatment is essential for valid inference.

# Introduction

The effective administration of federal disaster recovery funds is central to advancing equitable and timely community recovery. In the United States, the Community Development Block Grant--Disaster Recovery (CDBG-DR) program has become one of the largest and most flexible sources of long-term recovery assistance following major disaster declarations. Yet despite its national importance, the pace at which grantees---states, counties, cities, and territories---obligate, disburse, and expend CDBG-DR funds varies markedly across jurisdictions and disaster events. Slow or inconsistent financial performance has been repeatedly linked to delayed housing reconstruction, prolonged displacement, uneven economic recovery, and persistent inequities for low-income households and historically marginalized communities [@gao2019; @hudoig2021].

A central challenge is that governmental "capacity" in disaster recovery is difficult to measure directly. Previous evaluations often rely on single-period financial indicators or case-based assessments, making it difficult to compare capacity across time, jurisdictions, and disaster contexts. The Quarterly Performance Reports (QPRs) submitted through HUD's Disaster Recovery Grant Reporting (DRGR) system offer a rare opportunity to construct longitudinal, comparable indicators. These reports consistently document obligated, disbursed, and expended amounts for each grantee and activity, providing rich panel data for assessing financial performance. However, raw financial quantities alone cannot capture the multidimensional nature of administrative capacity, which includes efficiency, timeliness, and internal process management.

A second methodological challenge, often overlooked in prior research, is the **right-censoring problem**: the majority of disaster recovery programs are still ongoing at the time of analysis. At a 95% completion threshold, only 26.3% of grantee-disaster pairs have valid completion times; the remainder are right-censored observations. Standard regression approaches that exclude incomplete programs lose most of the sample and may introduce selection bias. Conversely, treating incomplete programs as "completed" at the observation date underestimates true completion times and can inflate apparent capacity effects.

**Survival analysis methods**---Cox Proportional Hazards and Accelerated Failure Time models---properly handle censored observations while utilizing the full sample. Using quarterly data on obligations, disbursements, and expenditures across 152 grantee-disaster pairs, capacity is measured through efficiency ratios observable for all programs regardless of completion status. The analysis proceeds in three stages: (1) descriptive analysis of capacity indicators across state and local governments; (2) survival model estimation to identify which capacity dimensions predict completion timing; and (3) robustness checks across government levels and model specifications.

Administrative capacity has long been recognized as a central determinant of public sector performance. In the public administration literature, capacity is typically conceptualized along three dimensions: analytical capacity (technical expertise and information processing), political capacity (coalition-building and stakeholder engagement), and operational capacity (resource management and implementation efficiency). For disaster recovery programs, operational capacity---particularly financial management---takes on heightened importance. The ability to efficiently convert federal obligations into disbursements and then into expenditures reflects an organization's procurement systems, environmental review processes, contracting capabilities, and internal coordination mechanisms. When capacity is strong, funds flow smoothly from federal authorization through to local implementation. When capacity is weak, administrative bottlenecks emerge, delaying projects and prolonging community recovery. Survival analysis provides a methodological bridge between capacity theory and empirical measurement: if capacity affects process efficiency, stronger capacity should reduce the time to complete program milestones. The censoring problem in disaster recovery data---most programs remain incomplete---makes survival methods particularly appropriate for testing capacity effects on completion timing.

# Literature Review


## Context: CDBG-DR administration and performance challenges


CDBG-DR has expanded substantially over the last two decades, distributing billions of dollars following major hurricanes, floods, wildfires, and other disasters. As a block-grant program, it affords grantees considerable flexibility, but this flexibility also introduces substantial administrative burden. Numerous evaluations have documented persistent delays in grant implementation, including slow procurement, environmental reviews, and program startups [@gao2019; @hud2020]. Specific bottlenecks frequently cited include National Environmental Policy Act (NEPA) compliance requirements, competitive procurement under federal regulations, duplication of benefits calculations that require coordination across multiple funding sources, and challenges in maintaining dedicated disaster recovery staff over multi-year implementation periods. Grantees differ considerably in their experience with federal recovery programs, staffing levels, contracting capacity, and administrative processes---all factors that influence the speed and effectiveness of fund deployment [@zhang2019; @howell2019].

State and local grantee experiences vary substantially. States such as Texas (post-Hurricane Harvey), Louisiana (post-Hurricane Katrina), and New Jersey (post-Superstorm Sandy) have managed billions in CDBG-DR allocations across multiple disasters, developing specialized staff and institutional knowledge. In contrast, local governments---including cities like Houston, New Orleans parishes, and New York City boroughs---often face more acute capacity constraints due to smaller administrative infrastructures and less prior experience with federal disaster programs. Grantees that served as recipients in previous disasters tend to exhibit faster initial obligation and disbursement rates, suggesting that institutional learning and retained expertise facilitate smoother program startup.

HUD requires quarterly reporting through the DRGR/QPR system, which captures obligated, disbursed, and expended amounts by activity and period. These data are widely used by auditors, HUD staff, and researchers to assess financial performance. However, prior analyses generally rely on descriptive summaries or aggregate comparisons rather than modeling the underlying determinants of administrative capacity using methods appropriate for time-to-event data.

## Characteristics of disaster recovery administration


In general, three consistent characteristics emerge in the fields of disaster recovery administration. (1) Significant cross-grantee variation: Grantees differ widely in their pace of obligations, disbursements, and expenditures. Studies document disbursement rates ranging from under 30% to over 80% of obligated funds within the first five years of program operation. Prior experience with CDBG-DR or similar programs is often associated with faster deployment [@gao2019]. (2) Differences between state and local administration: Comparative analyses find that state agencies typically possess larger administrative staff and more established procurement systems, while local governments may have stronger connections to affected communities but face greater capacity constraints [@peacock2022; @gerber2022]. Evidence on relative performance remains mixed due to methodological limitations and inconsistent measures. (3) Administrative processes strongly influence timelines: Delays often stem from procurement rules, environmental reviews, inconsistent staffing, and challenges in coordinating across agencies [@smith2021]. Procurement bottlenecks and environmental compliance procedures can extend project timelines by months or even years, independent of disaster severity. Institutional capacity---not only disaster severity---plays a critical role in shaping the speed of recovery.

Despite substantial progress, key gaps persist in the literature: (1) No studies properly account for the right-censoring inherent in ongoing recovery programs. Excluding incomplete programs or treating observation time as completion time introduces bias. (2) Survival analysis---a well-established methodology for time-to-event data---has not been applied to CDBG-DR completion timing.

## Methods used in prior scholarship


Research on disaster recovery administration has primarily used three methodological approaches. (1) Descriptive and programmatic analyses: HUD and the Government Accountability Office (GAO) frequently publish summaries highlighting differences in fund utilization rates across grantees [@gao2019; @hud2020]. These analyses reveal substantial variation but do not explain the institutional factors that drive performance. (2) Regression-based and panel analyses: Many public administration and planning studies use regression or panel models to examine how staffing, experience, governance type, or disaster severity influence implementation speed [@pipa2020; @gerber2022]. However, these models typically exclude incomplete programs, losing substantial sample size. (3) Case studies and qualitative evaluations: Qualitative studies offer deep insights into administrative processes but are not designed for comparative or longitudinal generalization.

Notably absent from the disaster recovery literature is the application of survival analysis---a well-established methodology for time-to-event data widely used in medical research, engineering reliability studies, and some areas of policy evaluation. Survival methods are specifically designed to handle right-censored observations, making them a natural fit for disaster recovery programs where most observations remain incomplete at the time of analysis. The 73.7% censoring rate in CDBG-DR data at the 95% completion threshold represents precisely the type of data structure for which survival analysis provides unbiased parameter estimates.

# Data and Research Design


## Data description and sources


The dataset, sourced from the quarterly performance reports (QPR) of HUD's Disaster Recovery Grant Reporting (DRGR) system, covers quarterly obligated, disbursed, and expended CDBG-DR funds across 78 states and local governments (grantees) and 18 disaster events spanning from 2003 to 2023. Each observation represents a grantee-disaster pair, with quarterly time series of fund flows.

- **Obligated QPR funds**: The amount of federal funding HUD has formally committed to a grantee for eligible disaster recovery activities. These funds are reserved but not yet spent.
- **Disbursed QPR funds**: The amount reimbursed or transferred from HUD to the grantee after expenditures are reported and approved.
- **Expended QPR funds**: The portion of obligated funds the grantee has already spent on approved recovery activities.

## Capacity indicators


Two primary capacity indicators can be observed for all programs regardless of completion status:

1. **Disbursement ratio** (Ratio_disbursed_to_obligated): The cumulative mean ratio of disbursed to obligated funds across quarters. Higher values indicate greater capacity to convert federal commitments into accessible funding.

2. **Expenditure ratio** (Ratio_expended_to_disbursed): The cumulative mean ratio of expended to disbursed funds. Higher values indicate greater capacity to deploy received funds into active projects.

These ratios are available for all 152 grantee-disaster pairs in the analysis sample.

## Duration and censoring


The primary outcome is **time to 95% completion**---the number of months from first obligation to achieving 95% expenditure of obligated funds. This outcome is subject to right-censoring:

| Threshold | Valid Observations | Percent of Sample |
|-----------|-------------------|-------------------|
| 30% complete | 95 | 60.9% |
| 50% complete | 89 | 57.1% |
| 70% complete | 75 | 48.1% |
| 90% complete | 58 | 37.2% |
| **95% complete** | **41** | **26.3%** |
| 100% complete | 15 | 9.6% |

At the 95% threshold, 73.7% of observations are right-censored---they have not yet completed. For survival analysis, the following are defined:

- **T** (time): Duration in months from first obligation to either completion or observation date
- **E** (event): Binary indicator; 1 = completed, 0 = censored

## Research design


The analysis proceeds in three stages:

1. **Descriptive analysis**: Compare capacity indicators across state and local governments, examining variation in disbursement and expenditure ratios.

2. **Survival analysis**: Estimate Cox Proportional Hazards and Accelerated Failure Time models to identify which capacity indicators predict completion timing while properly accounting for censored observations.

3. **Sensitivity analysis**: Test robustness across different completion thresholds and model specifications.

# Methodology: Survival Analysis


## Why survival analysis?


Traditional approaches to analyzing disaster recovery timing face a fundamental problem: most programs are still ongoing. Two common but problematic solutions are:

1. **Listwise deletion**: Exclude incomplete programs. This discards 74% of observations, severely reducing statistical power and potentially introducing selection bias if completion is correlated with capacity.

2. **Right-censoring as complete**: Treat observation time as completion time for incomplete programs. This underestimates true completion times and artificially inflates apparent capacity effects.

Survival analysis provides a principled solution. Originally developed for medical research where patients may be lost to follow-up, these methods properly handle censored observations by incorporating the information that a program has *not yet* completed without treating observation time as completion time.

## Cox Proportional Hazards model


The Cox model estimates the hazard function---the instantaneous probability of completion at time $t$ given survival to that point:

$$h(t|X) = h_0(t) \exp(\beta_1 X_1 + \beta_2 X_2 + \cdots)$$

Where:

- $h_0(t)$ is the baseline hazard (left unspecified)
- $X_1, X_2, \ldots$ are capacity predictors
- $\exp(\beta_i)$ gives the hazard ratio for a one-unit increase in $X_i$

A hazard ratio > 1 indicates faster completion; HR < 1 indicates slower completion.

The Cox model makes no assumptions about the baseline hazard distribution, only that hazards are proportional across covariate levels. This semi-parametric approach is robust to misspecification of the time distribution.

## Accelerated Failure Time models


AFT models directly model the log of completion time:

$$\log(T) = \mu + \beta_1 X_1 + \beta_2 X_2 + \cdots + \sigma \epsilon$$

Where $\epsilon$ follows a specified distribution (Weibull, lognormal, or log-logistic). The key output is the **time ratio**: $\exp(\beta_i)$ indicates the multiplicative effect on completion time for a one-unit increase in $X_i$.

A time ratio < 1 indicates faster completion (reduced time); TR > 1 indicates slower completion.

This analysis compares three distributions:

- **Weibull**: Hazard monotonically increasing or decreasing
- **Lognormal**: Hazard peaks then decreases
- **Log-logistic**: Similar to lognormal with different tail behavior

Model selection is based on AIC (lower is better) and concordance index (higher is better).

## Model evaluation


Model fit is assessed using:

1. **Concordance index (C-index)**: Proportion of pairs where the observation with higher predicted risk has shorter actual survival. C = 0.5 is random; C > 0.7 is acceptable; C > 0.8 is good.

2. **AIC**: For comparing AFT distribution choices.

3. **Coefficient significance**: Wald tests for individual predictors.

# Results


## Descriptive analysis of capacity indicators


### State governments


State governments managing CDBG-DR funds across 2003-2023 disasters show substantial variation in administrative capacity:

- **Disbursement ratios**: Generally clustered between 0.2 and 0.7, indicating that states typically disburse 20-70% of obligated funds per quarter on average.
- **Expenditure ratios**: Consistently high (0.8-1.5), showing that once funds are disbursed, states effectively expend them.

The pattern reveals a critical insight: states with higher disbursement ratios (above 0.6) tend to be those with prior disaster recovery experience---jurisdictions that have retained specialized staff and established standardized procurement procedures. States managing their first major CDBG-DR allocation often cluster in the lower range (0.2-0.4), suggesting that program startup and administrative systems development constitute the primary bottleneck. The disbursement challenge reflects the complexity of federal compliance requirements: environmental review processes, procurement regulations, and duplication of benefits calculations create administrative hurdles that must be navigated before funds can flow to implementing agencies and contractors.

In contrast, expenditure ratios show remarkable consistency. Once funds move from obligations to disbursements, states demonstrate uniformly strong performance in converting disbursed amounts to actual expenditures. This pattern indicates that the constraint is not in executing projects once they are underway, but in initiating them---clearing the administrative requirements necessary to disburse funds in the first place. The primary capacity challenge therefore lies in **timely disbursement**, not expenditure efficiency.

![Illustration of quarterly average ratios of disbursed/obligated and expended/disbursed funds across different state governments and 2003-2023 disaster events.](figures/fig_08_state_avg_ratios_2003_2023.png){#fig-state-ratios-2003-2023}


### Local governments


Local governments exhibit capacity patterns distinct from their state-level counterparts:

- **Disbursement ratios**: Lower and more tightly clustered than states, typically below 0.5.
- **Expenditure ratios**: Similarly high (0.8-1.5), showing strong expenditure performance once funds are received.

Disaster-specific patterns reveal important variation. Following Superstorm Sandy, urban localities such as New York City and New Jersey shore communities demonstrated disbursement challenges stemming from dense housing markets, complex land use regulations, and high real estate costs that complicated buyout programs. After Hurricane Harvey, Houston and surrounding Gulf Coast cities faced procurement bottlenecks when trying to rapidly scale housing repair programs to meet unprecedented demand. California wildfires affecting rural mountain communities exposed even more acute capacity constraints: smaller jurisdictions lacked dedicated disaster recovery staff and struggled to navigate federal environmental review requirements for rebuilding in fire-prone areas.

The state-local comparison reveals a systematic capacity gap. Local governments' disbursement ratios average approximately 15-20 percentage points lower than state governments managing comparable disasters. This differential likely reflects structural factors: cities and counties typically operate with smaller administrative staffs, have less experience with federal disaster programs, and must coordinate recovery efforts while simultaneously managing routine municipal services. Smaller jurisdictions may lack specialized procurement officers, environmental compliance staff, or dedicated grant administrators---positions that state agencies can more readily maintain across disaster cycles.

Despite these early-stage challenges, local governments demonstrate equally strong expenditure performance once funds are disbursed. This suggests that the capacity constraint operates primarily at the program initiation phase---navigating federal requirements, establishing systems, and securing environmental clearances. Once projects are approved and contracts executed, local implementation proceeds efficiently. The policy implication is clear: technical assistance and capacity-building resources should target early-stage administrative processes, particularly for local governments managing their first major disaster allocation.

![Illustration of quarterly average ratios of disbursed/obligated and expended/disbursed funds across different local governments and 2003-2023 disaster events.](figures/fig_09_local_avg_ratios_2003_2023.png){#fig-local-ratios-2003-2023}


## Survival analysis results


### Cox Proportional Hazards model


Using the full sample of 152 grantee-disaster pairs with proper censoring treatment:

| Predictor | Hazard Ratio | 95% CI | p-value |
|-----------|-------------|--------|---------|
| **Disbursement ratio** | **4.367** | [1.53, 12.48] | **0.006** |
| Expenditure ratio | 0.958 | [0.81, 1.14] | 0.626 |

**Key findings:**

1. **Disbursement capacity strongly predicts completion**: A one-unit increase in the disbursement ratio increases the completion hazard by 337%. Programs with higher disbursement capacity complete significantly faster.

2. **Expenditure capacity does not independently predict completion**: The expenditure ratio shows no significant effect (p = 0.626). Once funds are disbursed, variations in expenditure efficiency do not differentiate completion timing.

3. **Model fit**: Concordance index = 0.65, indicating acceptable discriminative ability.

### Accelerated Failure Time models


AFT models provide complementary estimates in terms of time ratios:

| Model | Predictor | Time Ratio | 95% CI | p-value |
|-------|-----------|-----------|--------|---------|
| **Lognormal** | **Disbursement ratio** | **0.157** | [0.06, 0.41] | **< 0.001** |
| Lognormal | Expenditure ratio | 1.008 | [0.78, 1.31] | 0.954 |
| Weibull | Disbursement ratio | 0.201 | [0.08, 0.51] | < 0.001 |
| Weibull | Expenditure ratio | 0.988 | [0.78, 1.25] | 0.917 |
| Log-logistic | Disbursement ratio | 0.178 | [0.07, 0.45] | < 0.001 |
| Log-logistic | Expenditure ratio | 1.019 | [0.80, 1.30] | 0.881 |

**Key findings:**

1. **Consistent across distributions**: The disbursement ratio effect is highly significant (p < 0.001) across all three AFT specifications.

2. **Substantial effect size**: A one-unit increase in disbursement ratio reduces expected completion time by 80-84%. Programs with higher disbursement capacity complete dramatically faster.

3. **Expenditure ratio non-significant**: Across all distributions, the expenditure ratio shows no significant effect (p > 0.88).

4. **Best-fitting model**: The lognormal distribution provides the best fit based on AIC.

### Interpretation

The survival analysis results point to a clear conclusion: **the critical capacity bottleneck in disaster recovery is the conversion of obligated funds to disbursements**. This early-stage administrative capacity---getting federal commitments into accessible funding---is the key predictor of recovery timelines.

In contrast, later-stage capacity---expending received funds---shows uniformly high performance across grantees and does not differentiate completion timing. This suggests that once jurisdictions overcome the disbursement hurdle, they can effectively deploy funds.

# Discussion


## Key findings and implications


The survival analysis results have direct implications for policy and practice:

**1. Disbursement capacity is the critical bottleneck.** The ability to convert obligated funds into disbursements is the strongest predictor of recovery completion timing. A one-unit increase in disbursement ratio increases completion hazard by 337% (Cox) or reduces expected completion time by 84% (AFT). To understand the practical significance of this effect: a program with an average disbursement ratio of 0.7 (converting 70% of obligations to disbursements) would be expected to complete approximately four times faster than an otherwise identical program with a ratio of 0.3. For communities awaiting housing reconstruction or infrastructure repair, this translates to years of difference in recovery timelines---a delay that disproportionately harms vulnerable populations facing prolonged displacement and economic disruption.

The mechanism underlying this disbursement bottleneck reflects the administrative complexity of federal disaster recovery programs. Disbursement---the act of making funds available to implementing agencies, contractors, and beneficiaries---requires completing multiple procedural gates. Grantees must conduct environmental reviews under NEPA, navigate competitive procurement requirements, calculate duplication of benefits across multiple funding sources (Federal Emergency Management Agency [FEMA], insurance, Small Business Administration [SBA] loans), secure local matching funds, and establish program management systems to track expenditures [@gao2019]. Each of these processes involves specialized expertise, inter-agency coordination, and substantial documentation. Programs with strong institutional capacity---experienced staff, established procedures, prior disaster recovery experience---navigate these requirements more quickly. Those lacking such capacity face extended delays at the program startup phase.

In federal-state-local partnerships, coordination challenges compound these difficulties. State grantees must develop allocation formulas, negotiate with local governments, and establish monitoring systems for subrecipients. Local grantees must interface with state and federal oversight while managing direct implementation. HUD and GAO evaluations consistently identify these coordination and procedural compliance requirements as the primary source of program delays, not deficiencies in project execution once approvals are secured.

**2. Expenditure capacity is uniformly high and non-predictive.** Once funds are disbursed, grantees across the sample show consistently strong expenditure performance. Variations in expenditure efficiency do not differentiate completion timing. This finding reveals an important asymmetry in the disaster recovery pipeline. Disbursement represents a gate that must be cleared through administrative processes; expenditure represents a flow that occurs once the gate is open. When funds reach contractors, service providers, and beneficiaries, they are spent efficiently. The challenge is not in executing projects but in initiating them---clearing the procedural requirements necessary to release funds in the first place.

This pattern suggests that technical capacity for project implementation---managing construction contracts, delivering services, processing beneficiary payments---is broadly adequate across grantees. The capacity gap lies in earlier-stage administrative functions: navigating federal regulations, establishing program infrastructure, and completing compliance documentation. Good news for policy: capacity-building interventions can target a specific, identifiable stage of the funding pipeline rather than attempting to strengthen all aspects of program administration simultaneously.

**3. Simple indicators suffice.** The single disbursement ratio provides strong prediction of completion timing. Complex composite measures or latent variable approaches are not required for effective monitoring. Tracking the cumulative mean ratio of disbursed to obligated funds offers HUD a parsimonious approach to identifying at-risk programs. Jurisdictions showing persistently low disbursement ratios in the first 12-24 months of program operation can be targeted for intensive technical assistance. This actionable insight supports the development of data-driven early-warning systems that could help HUD allocate technical assistance resources more strategically and intervene before programs experience multi-year delays.

## Limitations


Several limitations should be acknowledged:

1. **Capacity indicators are proxies**: Disbursement and expenditure ratios reflect administrative outputs, not the underlying organizational factors---staffing levels, procurement expertise, experience with federal programs, political commitment, and organizational culture---that produce these outputs. Ideal capacity measures would capture staff-to-grant ratios, years of experience among key personnel, procurement processing times, and technical skill assessments. However, these data are not systematically collected across grantees. Administrative data limitations prevent direct measurement of the organizational characteristics that drive performance. The ratios used here serve as observable markers of underlying capacity, but cannot identify which specific organizational attributes require strengthening.

2. **Endogeneity**: A fundamental challenge in capacity research is reverse causality: do strong capacity indicators cause faster completion, or do fast-moving programs mechanically generate higher ratios? A program that obligates and spends rapidly would naturally show high cumulative ratios. This analysis mitigates endogeneity by using time-averaged ratios computed over the full observation period, reducing the mechanical correlation between any single quarter's performance and overall completion timing. Additionally, the survival framework models time-to-event rather than cross-sectional outcomes, which partially addresses timing confounds. Nonetheless, residual endogeneity likely remains. Future work employing instrumental variable approaches---such as using prior disaster experience or initial staffing levels as instruments for capacity indicators---could more definitively establish causal relationships.

3. **Heterogeneity**: The analysis estimates average effects across all grantee-disaster pairs, but capacity effects may vary substantially by context. Hurricane recovery in coastal urban areas may differ from wildfire recovery in rural mountain communities. Large state agencies managing billion-dollar allocations may respond differently to capacity constraints than small counties managing their first federal disaster grant. Infrastructure programs may face different bottlenecks than housing programs. Although robustness checks (Appendix C) explore state versus local government differences, sample size limitations prevent more granular subgroup analyses. Effect heterogeneity across disaster types, program sizes, and community characteristics likely exists but cannot be fully characterized with 152 observations.

4. **External validity**: Results are specific to CDBG-DR and may not generalize to other federal disaster recovery programs or non-disaster federal grants. CDBG-DR's block grant structure, multi-year timelines, and heavy regulatory compliance requirements create a particular administrative environment. FEMA Public Assistance, SBA disaster loans, and HUD's standard CDBG program each involve different administrative processes, funding mechanisms, and capacity requirements. Caution is warranted in extrapolating these findings to other contexts.

## Future directions


Future research could extend this work in several methodologically and practically important directions.

**Predictive early-warning systems** represent the most immediately actionable extension. Operationalizing these findings requires translating survival model parameters into real-time monitoring tools that HUD program managers can use to identify at-risk grantees. By tracking disbursement trajectories during the first 12-24 months of program operation, prediction algorithms could flag jurisdictions likely to experience extended delays, enabling targeted technical assistance before problems become entrenched. Such systems could integrate additional risk factors---grant size, prior experience, disaster severity, government type---to produce risk scores that prioritize intervention resources. Prototype dashboards visualizing disbursement trends and completion probabilities could provide program officers with actionable intelligence for strategic capacity-building investments.

**Experimental evaluations of technical assistance interventions** would move beyond observational analysis to test what works in building administrative capacity. Randomized controlled trials could evaluate specific interventions: intensive procurement training programs, environmental review streamlining protocols, dedicated disaster recovery staffing grants, or peer mentorship matching experienced grantees with first-time recipients. By randomizing capacity-building treatments across comparable grantees, such studies would establish causal evidence on which interventions effectively reduce disbursement bottlenecks and accelerate recovery. Cost-effectiveness analysis would inform resource allocation decisions: which capacity investments generate the largest improvements in completion timing per dollar spent?

**Longitudinal capacity development studies** would track how grantees' administrative capabilities evolve across multiple disaster cycles. Do jurisdictions that manage Hurricane Katrina recovery in 2005 perform better when managing Hurricane Harvey recovery in 2017? How quickly does institutional knowledge decay when disaster recovery staff turn over or move to other positions? What organizational structures---dedicated disaster recovery offices, cross-trained staff, documented procedures---best preserve capacity across disaster cycles? Panel data following the same grantees through multiple events could reveal learning curves, institutional memory effects, and optimal capacity maintenance strategies.

**Enhanced data collection** would address the proxy measurement limitations identified above. Systematic collection of administrative process data---staffing levels, procurement timelines, environmental review durations, technical assistance utilization---would enable more granular analysis of capacity mechanisms. Structured interviews with grantee administrators could identify specific bottlenecks and successful strategies. Linking QPR financial data with HUD's technical assistance records would reveal which support services correlate with improved performance. Better measurement would support both research and practice, enabling evidence-based capacity building rather than generic technical assistance.

# Conclusions


Survival analysis of 152 grantee-disaster pairs from the CDBG-DR program reveals that **disbursement capacity---the ability to convert obligated funds to disbursements---significantly predicts completion timing** (HR = 4.37, p = 0.006). Higher disbursement capacity reduces expected completion time by 80-84%. In contrast, expenditure capacity shows no predictive value.

These findings carry broader significance for disaster equity and climate adaptation. Slow recovery perpetuates social and economic disparities. When administrative bottlenecks delay housing reconstruction and infrastructure repair, vulnerable populations---low-income households, communities of color, elderly residents---face prolonged displacement, economic hardship, and health impacts. Administrative capacity directly affects distributional outcomes: jurisdictions with weak capacity impose extended delays on populations least able to absorb them. As climate change drives increasing disaster frequency and severity, CDBG-DR allocations continue to grow, making effective administration progressively more consequential. Building systematic capacity---through dedicated staffing, standardized procedures, and institutional learning across disaster cycles---represents a critical infrastructure investment for climate adaptation. Without addressing administrative bottlenecks, even generous federal disaster appropriations will fail to deliver timely assistance to affected communities.

These findings have direct implications for disaster recovery policy:

1. **Target early-stage capacity**: Technical assistance should focus on procurement training programs, environmental review streamlining procedures, and dedicated disaster recovery staffing. HUD should differentiate support by government type: local governments require more intensive early-stage assistance given their systematically lower disbursement capacity, while state agencies need coordination and oversight tools for managing subrecipient networks. Concrete interventions include procurement officer training, standardized environmental review templates, and funding for dedicated grant administrator positions during the critical first 24 months of program operation.

2. **Use simple monitoring metrics**: HUD should implement quarterly disbursement ratio dashboards that flag programs falling below threshold performance levels (e.g., disbursement ratios below 0.4 in the first two years). Automated early-warning systems can identify at-risk programs for targeted intervention before delays become entrenched. The parsimonious nature of this indicator---a single ratio rather than complex composite measures---makes it operationally feasible for routine program management.

3. **Incorporate censoring in evaluation**: With 73.7% of CDBG-DR programs incomplete at the 95% threshold, methods that properly handle censored observations are essential for valid inference. Program evaluations and performance assessments should adopt survival analysis approaches rather than excluding incomplete programs or treating observation time as completion time. This methodological shift would enable more accurate benchmarking of grantee performance and more valid identification of capacity determinants.

Disbursement efficiency emerges as the critical bottleneck in disaster recovery administration. By accelerating early-stage fund conversion, jurisdictions can substantially reduce program completion timelines and speed assistance to affected communities.

# References {.unnumbered}

::: {#refs}
:::
