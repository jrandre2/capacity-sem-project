# Synthetic Peer Review Process

**Related**: [MANUSCRIPT_REVISION_CHECKLIST.md](MANUSCRIPT_REVISION_CHECKLIST.md) | [reviews/README.md](reviews/README.md)
**Status**: Active
**Last Updated**: 2025-12-26

---

## Overview

This project uses synthetic peer reviews generated by LLMs (Claude, GPT-4) to stress-test the manuscript before submission to **Public Administration Review (PAR)**. This proactive approach helps:

1. Identify methodological gaps before reviewers do
2. Strengthen robustness checks and diagnostics
3. Improve manuscript clarity and practitioner relevance
4. Anticipate likely reviewer concerns
5. Ensure PAR style compliance (word limits, Evidence for Practice, etc.)

---

## Workflow

### Step 1: Generate Review

```bash
python src/pipeline.py review_new --focus par_general
```

Provide the LLM with:
- The manuscript abstract and key results
- Methodology description (survival analysis)
- Main tables and figures
- Request a detailed critique using the PAR-specific prompt

### Step 2: Triage

Classify each review point using the status categories below.

```bash
# After manual triage, check status
python src/pipeline.py review_status
```

### Step 3: Track

Document responses in:
- `manuscript_quarto/REVISION_TRACKER.md` - Detailed tracking
- `doc/MANUSCRIPT_REVISION_CHECKLIST.md` - High-level summary

### Step 4: Implement

For VALID - ACTION NEEDED items:
1. Create implementation plan
2. Modify code/run new analyses as needed
3. Update manuscript text
4. Verify changes address the concern

### Step 5: Verify

```bash
python src/pipeline.py review_verify
```

Complete verification checklist before closing review cycle.

---

## Status Classifications

| Status | Description | Action |
|--------|-------------|--------|
| **VALID - ACTION NEEDED** | Legitimate concern requiring changes | Implement fix, document response |
| **ALREADY ADDRESSED** | Concern already handled in manuscript | Point to existing section |
| **BEYOND SCOPE** | Valid but deferred (data unavailable, future work) | Document reason, note for future |
| **INVALID** | Reviewer misunderstanding or error | Clarify if manuscript is ambiguous |

### Detailed Classification Guidance

#### VALID - ACTION NEEDED
The concern has merit and should be addressed. Document:
- What specific changes will be made
- Which files will be modified
- How to verify the fix

#### ALREADY ADDRESSED
The concern is valid but already handled. Document:
- Specific section/line references where addressed
- Consider if current text is clear enough
- No code changes needed

#### BEYOND SCOPE
The concern is valid but cannot be addressed in current revision. Document:
- Why it's beyond scope (data unavailable, computational constraints, etc.)
- How it might be addressed in future work
- Any partial mitigation already in place

#### INVALID
The reviewer's concern is based on a misunderstanding. Document:
- What the reviewer misunderstood
- Correct interpretation
- Whether manuscript clarity should be improved

---

## Focus-Specific Prompt Templates

### PAR General (Comprehensive Review)

> "Act as a peer reviewer for **Public Administration Review (PAR)**, the top journal in public administration. PAR emphasizes:
> 1. Relevance to practitioners and public managers
> 2. Evidence-based policy recommendations
> 3. Methodological rigor appropriate to the research question
> 4. Clear, accessible writing for multidisciplinary audience
>
> **Manuscript Context**:
> - Title: Government Capacity and Disaster Recovery: A Survival Analysis of CDBG-DR Fund Management
> - Method: Survival analysis (Cox PH, AFT) of 152 grantee-disaster pairs
> - Key Finding: Disbursement capacity predicts completion timing (HR=4.37, p=0.006)
> - Contribution: First application of survival analysis to disaster recovery administration
>
> **Review the manuscript for**:
>
> **1. Practitioner Relevance & Evidence for Practice**
> - Does the Evidence for Practice section provide actionable insights?
> - Are policy recommendations specific and implementable?
> - Would HUD program managers find this useful?
>
> **2. Methodological Appropriateness**
> - Is survival analysis justified for this research question?
> - Are Cox PH and AFT models appropriate?
> - Is the 95% completion threshold defensible?
> - Are capacity indicators (disbursement/expenditure ratios) valid proxies?
>
> **3. Threats to Validity**
> - Endogeneity: Do faster programs cause higher ratios, or vice versa?
> - Selection bias: Are incomplete programs systematically different?
> - Measurement error: How reliable are QPR self-reports?
> - External validity: Does CDBG-DR generalize to other programs?
>
> **4. Statistical Rigor**
> - Is N=152 adequate for survival analysis?
> - Are proportional hazards assumptions tested?
> - Are standard errors robust?
> - Are effect sizes practically meaningful?
>
> **5. Robustness & Heterogeneity**
> - Are findings robust to specification changes?
> - Do effects vary by disaster type (hurricanes vs wildfires)?
> - Do effects vary by government type (state vs local)?
> - Are outliers handled appropriately?
>
> **6. Literature & Theory**
> - Does the administrative capacity framework add value?
> - Is the survival analysis novelty overstated?
> - Are prior disaster recovery studies properly cited?
> - Is the contribution to public administration clear?
>
> **7. Presentation & Clarity**
> - Is the abstract clear and compelling?
> - Are figures/tables well-designed?
> - Is the discussion balanced (not overclaiming)?
> - Are limitations honestly acknowledged?
> - Is the writing accessible to non-specialists?
>
> **8. PAR-Specific Issues**
> - Word count under 8,000? (currently ~7,565)
> - Evidence for Practice section strong?
> - Policy recommendations actionable?
> - No "this study" metacommentary? (verified clean)
>
> **Format your review as**:
>
> ## MAJOR COMMENTS
> 1. [Issue title]
>    - Concern: [description]
>    - Severity: [CRITICAL | HIGH | MODERATE]
>    - Recommendation: [specific action]
>
> ## MINOR COMMENTS
> 1. [Issue]
> 2. [Issue]
>
> ## OVERALL ASSESSMENT
> - Recommendation: [ACCEPT | MINOR REVISIONS | MAJOR REVISIONS | REJECT]
> - Strengths: [3-5 key strengths]
> - Weaknesses: [3-5 key weaknesses]
> - Confidence: [HIGH | MEDIUM | LOW] (in your assessment)
>
> Be critical but fair. PAR rejects ~90% of submissions, so high standards apply."

### Methods Focus (Deep Methodological Review)

> "Act as a methodologist reviewing a manuscript for Public Administration Review.
>
> Focus exclusively on the **survival analysis methodology** used to analyze CDBG-DR disaster recovery completion timing.
>
> **Evaluate**:
>
> **1. Model Specification**
> - Are Cox Proportional Hazards and AFT Lognormal models appropriate?
> - Are covariates correctly specified?
> - Are proportional hazards assumptions tested and met?
> - Is the baseline hazard appropriately modeled?
>
> **2. Censoring Handling**
> - Is right-censoring (73.7% of sample) appropriately handled?
> - Is the 95% completion threshold justified?
> - Are informative censoring concerns addressed?
>
> **3. Sample Size Adequacy**
> - Is N=152 (40 events) adequate for the model complexity?
> - Are confidence intervals appropriately wide given sample size?
> - Should simpler models be considered?
>
> **4. Capacity Indicator Validity**
> - Are disbursement/expenditure ratios valid capacity proxies?
> - Are cumulative mean ratios appropriate (vs. other aggregations)?
> - Is measurement error discussed?
>
> **5. Endogeneity**
> - Is reverse causality adequately addressed?
> - Are instrumental variables or sensitivity analyses needed?
> - Are lagged capacity measures explored?
>
> **6. Robustness**
> - Are alternative specifications tested?
> - Are outliers handled appropriately?
> - Are results stable across subgroups?
>
> **7. Diagnostics**
> - Are residual diagnostics shown?
> - Are influential observations identified?
> - Are model fit statistics reported?
>
> Format your response with numbered major and minor methodological concerns."

### Policy Focus (Practitioner Relevance)

> "Act as a public administration practitioner reviewing a manuscript for PAR.
>
> You are a HUD program manager or state/local disaster recovery coordinator. Evaluate this manuscript for **practical relevance and actionability**.
>
> **Evaluate**:
>
> **1. Evidence for Practice Section**
> - Are the bullet points specific and actionable?
> - Would practitioners understand how to apply these insights?
> - Are recommendations feasible given real-world constraints?
>
> **2. Policy Recommendations**
> - Are recommendations concrete (not generic)?
> - Do they specify WHO should do WHAT?
> - Are timelines and resource requirements realistic?
> - Are political/institutional barriers acknowledged?
>
> **3. Generalizability**
> - Do findings apply beyond CDBG-DR to other disaster programs?
> - Would this help with FEMA, EDA, or state programs?
> - Are context-specific limitations clear?
>
> **4. Practitioner Accessibility**
> - Is the writing clear for non-technical audiences?
> - Are key findings summarized effectively?
> - Are technical details appropriately placed in appendices?
>
> **5. Real-World Examples**
> - Are specific disasters and jurisdictions used effectively?
> - Do examples resonate with practitioner experiences?
> - Are success stories and failures both represented?
>
> **6. Implementation Barriers**
> - Are political, legal, and resource constraints discussed?
> - Are change management issues addressed?
> - Are capacity-building recommendations included?
>
> Format your response as if writing a memo to the author about improving practitioner relevance."

### Clarity Focus (Presentation & Writing)

> "Act as a writing and communication expert reviewing a manuscript for PAR.
>
> Focus on **presentation quality, clarity, and accessibility** for PAR's multidisciplinary audience.
>
> **Evaluate**:
>
> **1. Abstract**
> - Is it under 150 words?
> - Does it convey the research question, method, finding, and implication?
> - Would a non-specialist understand the contribution?
>
> **2. Introduction**
> - Is the research question clear within the first page?
> - Is the motivation compelling?
> - Is the structure signposted?
>
> **3. Literature Review**
> - Is the theoretical framework accessible?
> - Are connections between concepts clear?
> - Is jargon minimized or explained?
>
> **4. Methods**
> - Can a non-specialist understand why survival analysis is appropriate?
> - Are technical details appropriately placed?
> - Are key assumptions explained in plain language?
>
> **5. Results**
> - Are tables and figures self-explanatory?
> - Are effect sizes interpreted substantively (not just statistically)?
> - Are key findings highlighted effectively?
>
> **6. Discussion**
> - Is the interpretation balanced?
> - Are limitations acknowledged honestly?
> - Are claims supported by evidence?
>
> **7. Conclusions**
> - Are policy implications clear?
> - Is the broader significance articulated?
> - Does it avoid overclaiming?
>
> **8. Overall Flow**
> - Are transitions smooth between sections?
> - Is the narrative coherent?
> - Are repetitions minimized?
>
> **9. PAR Style**
> - No "this study" self-references?
> - Chicago Author-Date citations correct?
> - Direct presentation of findings?
>
> Format your response with specific line-level and structural suggestions."

---

## Response Documentation Template

```markdown
## Review #[N] Response - [DATE]

### Summary Statistics

| Category | Total | Addressed | Beyond Scope | Pending |
|----------|-------|-----------|--------------|---------|
| Major Comments | X | X | X | X |
| Minor Comments | X | X | X | X |

### Major Comments

#### Comment 1: [Title]
**Status**: [VALID - ACTION NEEDED | ALREADY ADDRESSED | BEYOND SCOPE | INVALID]

**Reviewer's Concern**: [Quote or paraphrase]

**Validity Assessment**: [Why this is valid/invalid]

**Response**: [What was done or why not addressed]

**Files Modified**: [List specific files]

---

### Verification Checklist

- [ ] All analyses run successfully (`python src/pipeline.py run_all`)
- [ ] Manuscript text updated
- [ ] Tables/figures reflect new results
- [ ] REVISION_TRACKER.md updated
- [ ] Quarto renders without errors (`./render_all.sh`)
- [ ] Word count still under 8,000 (`wc -w manuscript_quarto/*.qmd`)
```

---

## CLI Commands

### Check Review Status
```bash
python src/pipeline.py review_status
```

### Start New Review Cycle
```bash
python src/pipeline.py review_new --focus par_general
python src/pipeline.py review_new --focus methods
python src/pipeline.py review_new --focus policy
python src/pipeline.py review_new --focus clarity
```

### Archive Completed Cycle
```bash
python src/pipeline.py review_archive
```

### Verify Current Cycle
```bash
python src/pipeline.py review_verify
```

### Generate Summary Report
```bash
python src/pipeline.py review_report
```

---

## File Organization

| File | Purpose |
|------|---------|
| `doc/SYNTHETIC_REVIEW_PROCESS.md` | This file - methodology guide |
| `doc/MANUSCRIPT_REVISION_CHECKLIST.md` | High-level revision status |
| `doc/reviews/README.md` | Index of all review cycles |
| `doc/reviews/archive/` | Completed review cycles |
| `manuscript_quarto/REVISION_TRACKER.md` | Detailed current review tracking |

---

## Best Practices

1. **Be honest about limitations**: If a concern can't be addressed, say so clearly
2. **Track everything**: Future reviews may raise similar concerns
3. **Verify changes**: Don't assume changes work - test them
4. **Update incrementally**: Small commits, clear messages
5. **Cross-reference**: Link between tracker, manuscript, and code
6. **Learn from patterns**: If multiple reviews raise same issue, it's important
7. **PAR compliance**: Always check word count, Evidence for Practice quality, and style
8. **State/local contrasts**: Ensure findings for both government types are clear

---

## PAR-Specific Verification Checklist

Before submission, verify:

- [ ] Word count ≤ 8,000 (including abstract, endnotes, references)
- [ ] Abstract ≤ 150 words
- [ ] Evidence for Practice section included (3-5 specific bullet points)
- [ ] No "this study" self-referential language
- [ ] Chicago Author-Date (16th edition) citations
- [ ] Full first names in references
- [ ] 12-point Times New Roman font
- [ ] Double-spaced
- [ ] 1-inch margins
- [ ] Blind review compliance (no author identification)

---

## Integration with CLAUDE.md

Key review findings should be incorporated into `CLAUDE.md` project instructions when they affect:
- Default methodology choices
- Interpretation caveats
- Robustness check requirements
- Documentation standards
- PAR style compliance rules

---

## Review History Template

| Review | Date | Focus | Major Comments | Addressed | Notes |
|--------|------|-------|----------------|-----------|-------|
| #1 | YYYY-MM-DD | PAR General | X | X | Initial pre-submission review |
| #2 | YYYY-MM-DD | Methods | X | X | Deep methodological dive |
| #3 | YYYY-MM-DD | Policy | X | X | Practitioner relevance check |

See [reviews/README.md](reviews/README.md) for detailed history and archived reviews.
